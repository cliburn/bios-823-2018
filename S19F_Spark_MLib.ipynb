{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spark MLLib\n",
    "\n",
    "- [Official documentation](http://spark.apache.org/docs/latest/ml-guide.html)\n",
    "- [PySpark documentation](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLLib Pipeline\n",
    "\n",
    "Generally, use of MLLIb for supervised and unsupervised learning follow some or all of the stages in the following template:\n",
    "\n",
    "- Get data\n",
    "- Pre-process the data\n",
    "- Convert data to a form that MLLib functions require (*)\n",
    "- Build a model\n",
    "- Optimize and fit the model to the data\n",
    "- Post-processing and model evaluation\n",
    "\n",
    "This is often assembled as a pipeline for convenience and reproducibility. This is very similar to what you would do with `sklearn`, except that MLLib allows you to handle massive datasets by distributing the analysis to multiple computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Spark and Spark SQL contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>151</td><td>application_1522938745830_0108</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://vcm-2168.oit.duke.edu:8088/proxy/application_1522938745830_0108/\">Link</a></td><td><a target=\"_blank\" href=\"http://vcm-2172.oit.duke.edu:8042/node/containerlogs/container_e19_1522938745830_0108_01_000001/user06021\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark MLLib imports\n",
    "\n",
    "The older `mllib` package works on RDDs. The newer `ml` package works on DataFrames. We will show examples using both, but it is more convenient to use the `ml` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.clustering import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised Learning\n",
    "----\n",
    "\n",
    "We saw this machine learning problem previously with `sklearn`, where the task is to distinguish rocks from mines using 60 sonar numerical features. We will illustrate some of the mechanics of how to work with MLLib - this is not intended to be a serious attempt at modeling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain data\n",
    "\n",
    "NAME: Sonar, Mines vs. Rocks\n",
    "\n",
    "SUMMARY: This is the data set used by Gorman and Sejnowski in their study\n",
    "of the classification of sonar signals using a neural network [1].  The\n",
    "task is to train a network to discriminate between sonar signals bounced\n",
    "off a metal cylinder and those bounced off a roughly cylindrical rock.\n",
    "\n",
    "SOURCE: The data set was contributed to the benchmark collection by Terry\n",
    "Sejnowski, now at the Salk Institute and the University of California at\n",
    "San Deigo.  The data set was developed in collaboration with R. Paul\n",
    "Gorman of Allied-Signal Aerospace Technology Center.\n",
    "\n",
    "See [description](https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://astro.temple.edu/~alan/sonar_all-data.txt'\n",
    "data = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 61)"
     ]
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0       1       2       3       4\n",
      "0  0.0200  0.0371  0.0428  0.0207  0.0954\n",
      "1  0.0453  0.0523  0.0843  0.0689  0.1183\n",
      "2  0.0262  0.0582  0.1099  0.1083  0.0974"
     ]
    }
   ],
   "source": [
    "print(data.iloc[:3, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       56      57      58      59 60\n",
      "0  0.0180  0.0084  0.0090  0.0032  R\n",
      "1  0.0140  0.0049  0.0052  0.0044  R\n",
      "2  0.0316  0.0164  0.0095  0.0078  R"
     ]
    }
   ],
   "source": [
    "print(data.iloc[:3, -5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['C%02d' % i for i in range(60)] + ['raw_label']\n",
    "df = spark.createDataFrame(data, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- C00: double (nullable = true)\n",
      " |-- C01: double (nullable = true)\n",
      " |-- C02: double (nullable = true)\n",
      " |-- C03: double (nullable = true)\n",
      " |-- C04: double (nullable = true)\n",
      " |-- C05: double (nullable = true)\n",
      " |-- C06: double (nullable = true)\n",
      " |-- C07: double (nullable = true)\n",
      " |-- C08: double (nullable = true)\n",
      " |-- C09: double (nullable = true)\n",
      " |-- C10: double (nullable = true)\n",
      " |-- C11: double (nullable = true)\n",
      " |-- C12: double (nullable = true)\n",
      " |-- C13: double (nullable = true)\n",
      " |-- C14: double (nullable = true)\n",
      " |-- C15: double (nullable = true)\n",
      " |-- C16: double (nullable = true)\n",
      " |-- C17: double (nullable = true)\n",
      " |-- C18: double (nullable = true)\n",
      " |-- C19: double (nullable = true)\n",
      " |-- C20: double (nullable = true)\n",
      " |-- C21: double (nullable = true)\n",
      " |-- C22: double (nullable = true)\n",
      " |-- C23: double (nullable = true)\n",
      " |-- C24: double (nullable = true)\n",
      " |-- C25: double (nullable = true)\n",
      " |-- C26: double (nullable = true)\n",
      " |-- C27: double (nullable = true)\n",
      " |-- C28: double (nullable = true)\n",
      " |-- C29: double (nullable = true)\n",
      " |-- C30: double (nullable = true)\n",
      " |-- C31: double (nullable = true)\n",
      " |-- C32: double (nullable = true)\n",
      " |-- C33: double (nullable = true)\n",
      " |-- C34: double (nullable = true)\n",
      " |-- C35: double (nullable = true)\n",
      " |-- C36: double (nullable = true)\n",
      " |-- C37: double (nullable = true)\n",
      " |-- C38: double (nullable = true)\n",
      " |-- C39: double (nullable = true)\n",
      " |-- C40: double (nullable = true)\n",
      " |-- C41: double (nullable = true)\n",
      " |-- C42: double (nullable = true)\n",
      " |-- C43: double (nullable = true)\n",
      " |-- C44: double (nullable = true)\n",
      " |-- C45: double (nullable = true)\n",
      " |-- C46: double (nullable = true)\n",
      " |-- C47: double (nullable = true)\n",
      " |-- C48: double (nullable = true)\n",
      " |-- C49: double (nullable = true)\n",
      " |-- C50: double (nullable = true)\n",
      " |-- C51: double (nullable = true)\n",
      " |-- C52: double (nullable = true)\n",
      " |-- C53: double (nullable = true)\n",
      " |-- C54: double (nullable = true)\n",
      " |-- C55: double (nullable = true)\n",
      " |-- C56: double (nullable = true)\n",
      " |-- C57: double (nullable = true)\n",
      " |-- C58: double (nullable = true)\n",
      " |-- C59: double (nullable = true)\n",
      " |-- raw_label: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform 60 features into MMlib vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['C%02d' % i for i in range(60)],\n",
    "    outputCol=\"raw_features\")\n",
    "output = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+---------+--------------------+\n",
      "|   C00|   C01|   C02|   C03|   C04|   C05|   C06|   C07|   C08|   C09|   C10|   C11|   C12|   C13|   C14|   C15|   C16|   C17|   C18|   C19|   C20|   C21|   C22|   C23|   C24|   C25|   C26|   C27|   C28|   C29|   C30|   C31|   C32|   C33|   C34|   C35|   C36|   C37|   C38|   C39|   C40|   C41|   C42|   C43|   C44|   C45|   C46|   C47|   C48|   C49|   C50|   C51|   C52|   C53|   C54|   C55|   C56|   C57|   C58|   C59|raw_label|        raw_features|\n",
      "+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+---------+--------------------+\n",
      "|  0.02|0.0371|0.0428|0.0207|0.0954|0.0986|0.1539|0.1601|0.3109|0.2111|0.1609|0.1582|0.2238|0.0645| 0.066|0.2273|  0.31|0.2999|0.5078|0.4797|0.5783|0.5071|0.4328| 0.555|0.6711|0.6415|0.7104| 0.808|0.6791|0.3857|0.1307|0.2604|0.5121|0.7547|0.8537|0.8507|0.6692|0.6097|0.4943|0.2744| 0.051|0.2834|0.2825|0.4256|0.2641|0.1386|0.1051|0.1343|0.0383|0.0324|0.0232|0.0027|0.0065|0.0159|0.0072|0.0167| 0.018|0.0084| 0.009|0.0032|        R|[0.02,0.0371,0.04...|\n",
      "|0.0453|0.0523|0.0843|0.0689|0.1183|0.2583|0.2156|0.3481|0.3337|0.2872|0.4918|0.6552|0.6919|0.7797|0.7464|0.9444|   1.0|0.8874|0.8024|0.7818|0.5212|0.4052|0.3957|0.3914| 0.325|  0.32|0.3271|0.2767|0.4423|0.2028|0.3788|0.2947|0.1984|0.2341|0.1306|0.4182|0.3835|0.1057| 0.184| 0.197|0.1674|0.0583|0.1401|0.1628|0.0621|0.0203| 0.053|0.0742|0.0409|0.0061|0.0125|0.0084|0.0089|0.0048|0.0094|0.0191| 0.014|0.0049|0.0052|0.0044|        R|[0.0453,0.0523,0....|\n",
      "|0.0262|0.0582|0.1099|0.1083|0.0974| 0.228|0.2431|0.3771|0.5598|0.6194|0.6333| 0.706|0.5544| 0.532|0.6479|0.6931|0.6759|0.7551|0.8929|0.8619|0.7974|0.6737|0.4293|0.3648|0.5331|0.2413| 0.507|0.8533|0.6036|0.8514|0.8512|0.5045|0.1862|0.2709|0.4232|0.3043|0.6116|0.6756|0.5375|0.4719|0.4647|0.2587|0.2129|0.2222|0.2111|0.0176|0.1348|0.0744| 0.013|0.0106|0.0033|0.0232|0.0166|0.0095| 0.018|0.0244|0.0316|0.0164|0.0095|0.0078|        R|[0.0262,0.0582,0....|\n",
      "+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+---------+--------------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "output.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features to have zero mean and unit standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = StandardScaler(withMean=True, withStd=True, \n",
    "                              inputCol='raw_features', \n",
    "                              outputCol='features')\n",
    "model = standardizer.fit(output)\n",
    "output = model.transform(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert label to numeric index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"raw_label\", outputCol=\"label\")\n",
    "indexed = indexer.fit(output).transform(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract only columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar = indexed.select(['features', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[-0.3985897356694...|  1.0|\n",
      "|[0.70184498705605...|  1.0|\n",
      "|[-0.1289179854363...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "sonar.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first fit a Gaussian Mixture Model with 2 components to the first 2 principal components of the data as an example of unsupervised learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca\")\n",
    "model = pca.fit(sonar)\n",
    "transformed = model.transform(sonar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            features|label|                 pca|\n",
      "+--------------------+-----+--------------------+\n",
      "|[-0.3985897356694...|  1.0|[-1.9165444107164...|\n",
      "|[0.70184498705605...|  1.0|[0.47896904316843...|\n",
      "|[-0.1289179854363...|  1.0|[-3.8499400285258...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "transformed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = transformed.select('pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = transformed.select('pca').rdd.map(lambda x: np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-1.91654441,  1.36759373]]), array([[ 0.47896904, -7.56812953]]), array([[-3.84994003, -6.42436107]])]"
     ]
    }
   ],
   "source": [
    "features.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(k=2, seed=123, featuresCol='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gmm.fit(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed2 = model.transform(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+--------------------+\n",
      "|            features|label|                 pca|prediction|         probability|\n",
      "+--------------------+-----+--------------------+----------+--------------------+\n",
      "|[-0.3985897356694...|  1.0|[-1.9165444107164...|         0|[0.62298055982439...|\n",
      "|[0.70184498705605...|  1.0|[0.47896904316843...|         0|[0.99997018397790...|\n",
      "|[-0.1289179854363...|  1.0|[-3.8499400285258...|         0|[0.83185871352854...|\n",
      "|[-0.8335441715294...|  1.0|[-4.5863546250792...|         1|[0.00590393467266...|\n",
      "+--------------------+-----+--------------------+----------+--------------------+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "transformed2.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get fitted Guassian parameters as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+---------------------------------------------------------------------------------+\n",
      "|mean                                   |cov                                                                              |\n",
      "+---------------------------------------+---------------------------------------------------------------------------------+\n",
      "|[1.45630530064182,-0.37037164792437044]|3.615993244446481   1.5178686905624637  \n",
      "1.5178686905624637  10.969615585723204  |\n",
      "|[-5.357055209406797,1.362421303455668] |7.039461019406999   3.6991498391254107  \n",
      "3.6991498391254107  10.078828243534241  |\n",
      "+---------------------------------------+---------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "model.gaussiansDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit a logistic regression model to the data as an example of supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[-0.3985897356694...|  1.0|\n",
      "|[0.70184498705605...|  1.0|\n",
      "|[-0.1289179854363...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "sonar.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `ml` for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to format expected by regression functions in `mllib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "sonar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = sonar.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[-1.0988663774039...|  1.0|\n",
      "|[-0.9727295910045...|  1.0|\n",
      "|[-0.9248846030599...|  1.0|\n",
      "|[-0.8770396151153...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 4 rows"
     ]
    }
   ],
   "source": [
    "train.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[-0.9031368812669...|  1.0|[-3.9291177051083...|[0.01928190985745...|       1.0|\n",
      "|[-0.8726900707567...|  1.0|[-4.1830133029604...|[0.01502333483134...|       1.0|\n",
      "|[-0.8335441715294...|  1.0|[0.47133208472102...|[0.61569899448073...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'areaUnderROC'"
     ]
    }
   ],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8369905956112853"
     ]
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the  `ml` pipeline\n",
    "\n",
    "We build a pipeline to preoprcess and fit a  logistic regression model to the original DataFrame. The pipeline stages consist of\n",
    "\n",
    "- Convert featrue columns in DataFrame into a vector of features \n",
    "- Scele features to have zero mean and unit standard deviation\n",
    "- Convert string labels into numeric labels\n",
    "- Reduce dimensionality using PCA with the first 5 PCs\n",
    "- Run logistic regression to predict the labels from the feature vector of 5 Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+----+------+------+------+------+------+------+-----+------+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+-----+------+---------+\n",
      "| C00|   C01|   C02|   C03|   C04|   C05|   C06|   C07|   C08|   C09|   C10|   C11|   C12|   C13|  C14|   C15| C16|   C17|   C18|   C19|   C20|   C21|   C22|  C23|   C24|   C25|   C26|  C27|   C28|   C29|   C30|   C31|   C32|   C33|   C34|   C35|   C36|   C37|   C38|   C39|  C40|   C41|   C42|   C43|   C44|   C45|   C46|   C47|   C48|   C49|   C50|   C51|   C52|   C53|   C54|   C55|  C56|   C57|  C58|   C59|raw_label|\n",
      "+----+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+----+------+------+------+------+------+------+-----+------+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+-----+------+---------+\n",
      "|0.02|0.0371|0.0428|0.0207|0.0954|0.0986|0.1539|0.1601|0.3109|0.2111|0.1609|0.1582|0.2238|0.0645|0.066|0.2273|0.31|0.2999|0.5078|0.4797|0.5783|0.5071|0.4328|0.555|0.6711|0.6415|0.7104|0.808|0.6791|0.3857|0.1307|0.2604|0.5121|0.7547|0.8537|0.8507|0.6692|0.6097|0.4943|0.2744|0.051|0.2834|0.2825|0.4256|0.2641|0.1386|0.1051|0.1343|0.0383|0.0324|0.0232|0.0027|0.0065|0.0159|0.0072|0.0167|0.018|0.0084|0.009|0.0032|        R|\n",
      "+----+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+----+------+------+------+------+------+------+-----+------+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+-----+------+-----+------+---------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = VectorAssembler(\n",
    "    inputCols=['C%02d' % i for i in range(60)],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "standardizer = StandardScaler(\n",
    "    withMean=True, \n",
    "    withStd=True, \n",
    "    inputCol='raw_features', \n",
    "    outputCol='features'\n",
    ")\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"raw_label\", \n",
    "    outputCol=\"label\"\n",
    ")\n",
    "pca = PCA(\n",
    "    k=5, \n",
    "    inputCol=\"features\", \n",
    "    outputCol=\"pca\"\n",
    ")\n",
    "lr = LogisticRegression(\n",
    "    featuresCol='features', \n",
    "    labelCol='label'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[transformer, standardizer, indexer, pca, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  1.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+"
     ]
    }
   ],
   "source": [
    "score = prediction.select(['label', 'prediction'])\n",
    "score.show(n=score.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7796610169491526"
     ]
    }
   ],
   "source": [
    "acc = score.rdd.map(lambda x: x[0] == x[1]).sum() / float(score.count())\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
