{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Overview of some concepts\n",
    "\n",
    "## What do we do with data?\n",
    "\n",
    "- What's the difference between unsupervised, semi-supervised, and supervised learning?\n",
    "  - Labels for none (unsupervised), some (semi-supervised) or all (supervised) samples\n",
    "- Why do we normalize data? Is it always necessary?\n",
    "  - Normalization may be necessary so that all features are weighted equally\n",
    "  - Necessary for models that use Euclidean metrics (e.g. multivariate regression), may not be necessary for models that look at one feature at a time (e.g decision tree).\n",
    "- How do we convert raw data into feature vectors?\n",
    "  - Depends on data types\n",
    "  - For example, conversion of text to vectors\n",
    "- How do we select features?\n",
    "  - Simple criteria that don't depend on outcomes (e.g. feature variance, removal of correlation such as PCA)\n",
    "  - Criteria that depend on outcomes (e.g. top 10 by p-value ranking in univariate testing) require incorporation into pipeline - i.e. each cross-validation needs a new set of top 10 to avoid biasing evaluation\n",
    "- How do we deal with categorical variables?\n",
    "  - Use of integers implies ordering which may be misleading\n",
    "  - Most common is the use of dummy or one-hot encoding\n",
    "  - Dummy encoding results in $n-1$ columns for $n$ categories\n",
    "  - One-hot encoding results in $n$ columns for $n$ categories - have to set intercept to zero to avoid collinearity\n",
    "  - Colinearity means that one or more variables can be expressed as a linear combination of the others - why is this a problem?\n",
    "- What is data augmentation?\n",
    "  - Using synthetic data to increase size of training set\n",
    "  - Most common in deep learning as the models have very high capacity\n",
    "- What is an unbalanced lanced data set?\n",
    "  - Distribution of classes is non-uniform\n",
    "  - Very, very common issue\n",
    "- How do we deal with an unbalanced data set?\n",
    "  - At least, need baseline evaluation of model that incorporates unbalance - for example 99% accuracy is not useful when 99% of the samples belong to a single class\n",
    "  - Use a evaluation metric that is not sensitive to imbalance (e.g. [Kappa](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html))\n",
    "  - Resample data set\n",
    "  - Decision tree algorithms are less sensitive to imbalanced data sets\n",
    "  - Penalize algorithm by increasing cost of mistake for minority classes\n",
    "  - Generate synthetic samples (data augmentation)\n",
    "  - Consider minor classes as single group and see if they can be detected with anomaly detection algorithms\n",
    "  - Also See [imbalanced-learn package](https://github.com/scikit-learn-contrib/imbalanced-learn)\n",
    "\n",
    "## What types of models are there?\n",
    "\n",
    "- What is a machine learning model?\n",
    "  - An implicit or explicit function that takes a vector (features) as input and returns an integer (classification) or reel number (regression)\n",
    "- What is the XOR problem?\n",
    "  - Find a function to separate two classes - class 1 at (0,0) and (1,1) and class 2 at (0,1) and (1,0)\n",
    "- How does increasing dimensionality allow a linear model to solve the XOR problem?\n",
    "  - For example if we created an extra feature $\\vert x^2 - y^2 \\vert$, we can solve the XOR problem with a linear model\n",
    "\n",
    "| Class |  x  |  y  | $\\vert x^2 - y^2 \\vert$ |\n",
    "| ----- | --- | --- | ----------------------- |\n",
    "| 0     | 0   | 0   | 0                       |\n",
    "| 1     | 0   | 1   | 1                       |\n",
    "| 1     | 1   | 0   | 1                       |\n",
    "| 0     | 1   | 1   | 0                       |\n",
    "\n",
    "- What's the difference between shallow and deep learning?\n",
    "  - Shallow learning is a term used by deep learning practitioners to refer to classical machine learning techniques\n",
    "  - Deep learning is sometimes used to refer to neural networks with many filters layers\n",
    "- What machine learning models do you know?\n",
    "  - Some of the major classes are\n",
    "    - Nearest neighbor\n",
    "    - Linear models\n",
    "    - Kernel methods such as SVM\n",
    "    - Tree-based\n",
    "    - Ensembles\n",
    "    - Neural networks\n",
    "- Can you briefly explain how each type of model works?\n",
    "- What is ensemble learning?\n",
    "- What is the difference between boosting and bagging?\n",
    "  - Both use bootstrap to generate N samples (sample with replacement) and train N machines\n",
    "  - Bagging (bootstrap aggregating) averages the weights over each machine (hence reducing variance and preventing overfitting)\n",
    "  - Random forest is like bagging, but we also select a random sample of features at each step - this forces the trees to be more different than with bagging\n",
    "  - Boosting describes a group of methods that give more weight to more effective machines automatically\n",
    "\n",
    "## How do we select a model?\n",
    "\n",
    "- What is a hyper-parameter?\n",
    "  - A different value of a hyper-parameter defines a different member of the same class of machine learning algorithms. For example, the degree of a polynomial is a hyperparameter while the coefficients are regular parameters\n",
    "- What is bias-variance trade-off\n",
    "  - Show derivation\n",
    "![img](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n",
    "- What is [regularization](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)?\n",
    "  - A statistical technique to reduce variance in the model\n",
    "- Why is regularization necessary?\n",
    "  - Reduces risk of over-fitting\n",
    "  - May also perform feature selection (L1 regularization)\n",
    "- How do we perform regularization?\n",
    "  - L1 and L2 examples\n",
    "  - L2: $\\lambda \\sum \\beta_i^2$\n",
    "  - L1: $\\lambda \\sum \\vert \\beta_i \\vert$\n",
    "  - Regularization as constrained optimization\n",
    "  - Why does L1 result in sparsity?\n",
    "- How and why do we perform cross-validation?\n",
    "  - Usually, to estimate out-of-sample errors when doing model selection\n",
    "  - $k$-fold cross-validation\n",
    "- What is leave-one-out-cross-validation (LOOCV)?\n",
    "  - Same as k-fold, when $k = n$  \n",
    "\n",
    "## How do we fit a model to data?\n",
    "\n",
    "- What is model capacity?\n",
    "  - Amount of complexity the model can encode\n",
    "  - If capacity is high relative to the amount of data, there is a risk that the model just \"memorizes\" the training set and fails to generalize\n",
    "- How do we use a loss function?\n",
    "  - In the model fitting, many ML algorithms optimize the model parameters with respect to the loss function\n",
    "- What types of loss functions are there?\n",
    "  - MSE, MAE, cross-entropy\n",
    "\n",
    "## How do we evaluate if our model is any good?\n",
    "\n",
    "- How do we evaluate a model?\n",
    "  - We need to define a performance metric to measure\n",
    "  - Make predictions on test set and calculate performance metric\n",
    "- What is a confusion matrix?\n",
    "  - A square matrix with true on columns and predicted on rows\n",
    "- How do we define accuracy from a confusion matrix?\n",
    "  - Trace of matrix over sum of all entires\n",
    "- How do we construct ROC and PRC curves?\n",
    "  - Plots to evaluate model for different values of a cutoff\n",
    "  - See earlier [lectures](http://people.duke.edu/~ccc14/bios-823-2018/S10_Anomaly_Detection.html#Precision-recall-and-ROC-curves)\n",
    "- What is a Bayes optimal classifier?\n",
    "  - Theoretical classifier that gives the Baye's error - i.e. all error is due to intrinsic noise\n",
    "- Why is it critical to evaluate our model on out-of-sample data?\n",
    "  - Predictions of in-sample data can be due to memorization rather than generalization\n",
    "\n",
    "## How do we improve ML performance?\n",
    "  \n",
    "- What is Baye's error?\n",
    "  - Irreducible error - error due to noise in system\n",
    "- How do we estimate the Baye's error?\n",
    "  - Generally not possible\n",
    "  - If it is a problem that humans are good at solving, the best human error may be a good approximation of the Baye's error\n",
    "- How can we diagnose underfitting?\n",
    "  - Underfitting implies training error is much larger than Baye's error\n",
    "  - Generally seen by reducing training error as we increase model capacity\n",
    "- What can we do if we are underfitting?\n",
    "  - Increase model complexity\n",
    "- How do we diagnose overfitting?\n",
    "  - Training error does not change but validation error increases as we increase model capacity\n",
    "- What can we do if we are overfitting?\n",
    "  - Increase data\n",
    "  - Use some form of regularization (L1, L2, bagging, dropout)\n",
    "  - Decrease model capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
