{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame(dict(a=range(6), b=list('aabbcc'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  0|  a|\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "|  3|  b|\n",
      "|  4|  c|\n",
      "|  5|  c|\n",
      "+---+---+"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  0|  a|\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "|  3|  b|\n",
      "|  4|  c|\n",
      "|  5|  c|\n",
      "+---+---+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * FROM df_table\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  2|  b|\n",
      "|  3|  b|\n",
      "+---+---+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * \n",
    "FROM df_table\n",
    "WHERE b='b'\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------------+\n",
      "|  b|avg|               std|\n",
      "+---+---+------------------+\n",
      "|  a|0.5|0.7071067811865476|\n",
      "|  b|2.5|0.7071067811865476|\n",
      "|  c|4.5|0.7071067811865476|\n",
      "+---+---+------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT b, mean(a) AS avg, std(a) AS std\n",
    "FROM df_table\n",
    "GROUP BY b\n",
    "ORDER BY b\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        function|\n",
      "+----------------+\n",
      "|     cardinality|\n",
      "|            cast|\n",
      "|            cbrt|\n",
      "|            ceil|\n",
      "|         ceiling|\n",
      "|            char|\n",
      "|     char_length|\n",
      "|character_length|\n",
      "|             chr|\n",
      "|        coalesce|\n",
      "+----------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SHOW FUNCTIONS \"c*\"\n",
    "''').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql('''\n",
    "SELECT a, b, (a, b) AS struct FROM df_table\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('df1_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "|  a|  b|struct|\n",
      "+---+---+------+\n",
      "|  0|  a|[0, a]|\n",
      "|  1|  a|[1, a]|\n",
      "|  2|  b|[2, b]|\n",
      "|  3|  b|[3, b]|\n",
      "|  4|  c|[4, c]|\n",
      "|  5|  c|[5, c]|\n",
      "+---+---+------+"
     ]
    }
   ],
   "source": [
    "df1_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  0|  a|\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "|  3|  b|\n",
      "|  4|  c|\n",
      "|  5|  c|\n",
      "+---+---+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT struct.a, struct.b FROM df1_table\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql('''\n",
    "SELECT b, collect_list(a) as list\n",
    "FROM df_table\n",
    "GROUP BY b\n",
    "ORDER BY b\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView('df2_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|  b|  list|\n",
      "+---+------+\n",
      "|  a|[0, 1]|\n",
      "|  b|[2, 3]|\n",
      "|  c|[4, 5]|\n",
      "+---+------+"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  b|  a|\n",
      "+---+---+\n",
      "|  a|  0|\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "|  c|  5|\n",
      "+---+---+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT b, explode(list) as a\n",
    "FROM df2_table\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.select('a', 'b', create_map('b', 'a').alias('map'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView('df3_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+\n",
      "|  a|  b|     map|\n",
      "+---+---+--------+\n",
      "|  0|  a|[a -> 0]|\n",
      "|  1|  a|[a -> 1]|\n",
      "|  2|  b|[b -> 2]|\n",
      "|  3|  b|[b -> 3]|\n",
      "|  4|  c|[c -> 4]|\n",
      "|  5|  c|[c -> 5]|\n",
      "+---+---+--------+"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+\n",
      "|  a|  b|map[c]|\n",
      "+---+---+------+\n",
      "|  4|  c|     4|\n",
      "|  5|  c|     5|\n",
      "+---+---+------+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT a, b, map['c'] FROM df3_table\n",
    "''').na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
